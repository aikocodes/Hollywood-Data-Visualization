{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b38e7cf3-bc79-4187-be1e-2772d617b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "\"\"\"\n",
    "    CS181DV Assignment 4: High-Performance Visualization with Big Data\n",
    "\n",
    "    Author: AIKO KATO\n",
    "\n",
    "    Date: 04/06/2025\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95d4362f-7520-48f9-aadf-a5be8402aa5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "# Function to read a specific chunk from a large tsv file into a dataframe\n",
    "def read_chunk(filename, start_pos: int, size: int):\n",
    "    # Read the header once to get column names\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        header = f.readline().strip().split('\\t')\n",
    "    data = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            # Skip lines before the chunk's start position (+1 for header)\n",
    "            if i < start_pos + 1:\n",
    "                continue\n",
    "            # Stop once we reach the end of the chunk\n",
    "            if i >= start_pos + 1 + size:\n",
    "                break\n",
    "            data.append(line.strip().split('\\t'))\n",
    "    # Convert list of rows to a dataframe with the correct column names\n",
    "    return pd.DataFrame(data, columns=header)\n",
    "\n",
    "# Function to divide the file into roughly equal-sized chunks\n",
    "def distribute_workload(total_size: int, num_chunks: int) -> List[Tuple[int, int]]:\n",
    "    base_chunk = total_size // num_chunks  # minimum size per chunk\n",
    "    extra = total_size % num_chunks  # extra rows to distribute evenly\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    for i in range(num_chunks):\n",
    "        size = base_chunk + (1 if i < extra else 0)\n",
    "        chunks.append((start, size))\n",
    "        start += size\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9daac39-06eb-403a-bb7d-cf7948500008",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "# Utility function to replace IMDB-style missing values (\"\\N\") with proper nulls\n",
    "def replace_missing(df):\n",
    "    return df.replace('\\\\N', pd.NA)\n",
    "\n",
    "# Cleaning function for title.basics.tsv\n",
    "def clean_basics(df):\n",
    "    df = replace_missing(df)\n",
    "    # Convert 'startYear' to numeric and drop nulls\n",
    "    df['startYear'] = pd.to_numeric(df['startYear'], errors='coerce')\n",
    "    df = df[df['startYear'].notnull()]\n",
    "    df['startYear'] = df['startYear'].astype('int16')  # I used ChatGPT for this line.\n",
    "    \n",
    "    # Convert 'isAdult' to int8 with fallback to 0\n",
    "    df['isAdult'] = pd.to_numeric(df['isAdult'], errors='coerce').fillna(0).astype('int8')\n",
    "    \n",
    "    # Handle missing runtimes using median, then cast to int16\n",
    "    df['runtimeMinutes'] = pd.to_numeric(df['runtimeMinutes'], errors='coerce')\n",
    "    df['runtimeMinutes'] = df['runtimeMinutes'].fillna(df['runtimeMinutes'].median()).astype('int16')  # I used ChatGPT for this line.\n",
    "    \n",
    "    # Add column to flag if 'isAdult' was originally missing\n",
    "    df['wasIsAdultMissing'] = df['isAdult'].isnull()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Cleaning function for title.ratings.tsv\n",
    "def clean_ratings(df):\n",
    "    df = replace_missing(df)\n",
    "    df['averageRating'] = pd.to_numeric(df['averageRating'], errors='coerce').astype('float32')  # I used ChatGPT for this line.\n",
    "    df['numVotes'] = pd.to_numeric(df['numVotes'], errors='coerce').astype('int32')  # I used ChatGPT for this line.\n",
    "    return df\n",
    "\n",
    "# Cleaning function for title.crew.tsv\n",
    "def clean_crew(df):\n",
    "    df = replace_missing(df)\n",
    "    df[\"directors\"] = df[\"directors\"].fillna(\"\").astype(str)\n",
    "    df[\"writers\"] = df[\"writers\"].fillna(\"\").astype(str)\n",
    "    return df\n",
    "\n",
    "# Cleaning function for name.basics.tsv\n",
    "def clean_names(df):\n",
    "    # Only replace missing values, keep all other columns as-is for flexibility\n",
    "    df = df.replace(\"\\\\N\", pd.NA)\n",
    "    return df  # Keep all columns for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d187629-4923-4b99-ac1d-91103426bf7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "# Core function to process large files in chunks and write them as parquet\n",
    "def process_large_file(filename: str, output_path: str, clean_func, chunk_size: int = 1_000_000):\n",
    "    print(f\"Processing: {filename}\")\n",
    "    \n",
    "    # Count the total number of rows (excluding header)\n",
    "    with open(filename, 'r', encoding='utf-8') as f:  # I used ChatGPT for this line.\n",
    "        total_lines = sum(1 for _ in f) - 1\n",
    "\n",
    "    # Divide total lines into manageable chunks\n",
    "    chunks = distribute_workload(total_lines, total_lines // chunk_size + 1)\n",
    "    dfs = []\n",
    "\n",
    "    for start, size in chunks:\n",
    "        print(f\"Chunk {start} → {start + size}\")\n",
    "        try:\n",
    "            # Read and clean each chunk\n",
    "            chunk_df = read_chunk(filename, start, size)\n",
    "            chunk_df = clean_func(chunk_df)\n",
    "            dfs.append(chunk_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in chunk {start}: {e}\")\n",
    "            break\n",
    "\n",
    "    # Concatenate all chunks and save as a Parquet file\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "    final_df.to_parquet(output_path)\n",
    "    print(f\"Saved cleaned file to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "040cb053-e342-4ffc-a4e6-a1759f744707",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "# Map each imdb file to its corresponding cleaning function\n",
    "files_to_clean = {\n",
    "    \"title.basics.tsv\": clean_basics,\n",
    "    \"title.ratings.tsv\": clean_ratings,\n",
    "    \"title.crew.tsv\": clean_crew,\n",
    "    \"name.basics.tsv\": clean_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30fd77f3-a03c-42d4-a4e5-ee120d30da30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: share/dataset/imdb/title.basics.tsv\n",
      "Chunk 0 → 941645\n",
      "Chunk 941645 → 1883290\n",
      "Chunk 1883290 → 2824934\n",
      "Chunk 2824934 → 3766578\n",
      "Chunk 3766578 → 4708222\n",
      "Chunk 4708222 → 5649866\n",
      "Chunk 5649866 → 6591510\n",
      "Chunk 6591510 → 7533154\n",
      "Chunk 7533154 → 8474798\n",
      "Chunk 8474798 → 9416442\n",
      "Chunk 9416442 → 10358086\n",
      "Chunk 10358086 → 11299730\n",
      "Saved cleaned file to: share/processed_data/processed_title.basics.parquet\n",
      "Processing: share/dataset/imdb/title.ratings.tsv\n",
      "Chunk 0 → 754818\n",
      "Chunk 754818 → 1509635\n",
      "Saved cleaned file to: share/processed_data/processed_title.ratings.parquet\n",
      "Processing: share/dataset/imdb/title.crew.tsv\n",
      "Chunk 0 → 966518\n",
      "Chunk 966518 → 1933036\n",
      "Chunk 1933036 → 2899554\n",
      "Chunk 2899554 → 3866072\n",
      "Chunk 3866072 → 4832590\n",
      "Chunk 4832590 → 5799108\n",
      "Chunk 5799108 → 6765626\n",
      "Chunk 6765626 → 7732143\n",
      "Chunk 7732143 → 8698660\n",
      "Chunk 8698660 → 9665177\n",
      "Chunk 9665177 → 10631694\n",
      "Saved cleaned file to: share/processed_data/processed_title.crew.parquet\n",
      "Processing: share/dataset/imdb/name.basics.tsv\n",
      "Chunk 0 → 934410\n",
      "Chunk 934410 → 1868820\n",
      "Chunk 1868820 → 2803230\n",
      "Chunk 2803230 → 3737640\n",
      "Chunk 3737640 → 4672050\n",
      "Chunk 4672050 → 5606460\n",
      "Chunk 5606460 → 6540869\n",
      "Chunk 6540869 → 7475278\n",
      "Chunk 7475278 → 8409687\n",
      "Chunk 8409687 → 9344096\n",
      "Chunk 9344096 → 10278505\n",
      "Chunk 10278505 → 11212914\n",
      "Chunk 11212914 → 12147323\n",
      "Chunk 12147323 → 13081732\n",
      "Chunk 13081732 → 14016141\n",
      "Saved cleaned file to: share/processed_data/processed_name.basics.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cell 6\n",
    "# Input/output directories\n",
    "data_path = \"share/dataset/imdb/\"\n",
    "output_path = \"share/processed_data/\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Run the pipeline for all files\n",
    "for fname, cleaner in files_to_clean.items():\n",
    "    input_file = os.path.join(data_path, fname)\n",
    "    out_file = os.path.join(output_path, f\"processed_{fname.replace('.tsv', '')}.parquet\")\n",
    "    process_large_file(input_file, out_file, cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81769903-0241-4dae-9cad-6804c5d29849",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Data Visualization)",
   "language": "python",
   "name": "dataviz_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
